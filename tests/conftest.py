"""
CrewAI Agentic Patterns æ¸¬è©¦é…ç½®

æä¾›æ¸¬è©¦ç’°å¢ƒè¨­å®šå’Œå…±ç”¨ fixturesï¼Œæ”¯æ´ï¼š
- å–®å…ƒæ¸¬è©¦
- æ•´åˆæ¸¬è©¦  
- æ•ˆèƒ½æ¸¬è©¦
- Agentic Pattern æ¸¬è©¦
"""

import pytest
import asyncio
import os
from typing import List, Dict, Any
from unittest.mock import Mock, MagicMock

from crewai import Agent, Task, Crew
from crewai.tools import BaseTool

# è¨­å®šæ¸¬è©¦ç’°å¢ƒè®Šæ•¸
os.environ.setdefault("OPENAI_API_KEY", "test-key")
os.environ.setdefault("ENVIRONMENT", "test")
os.environ.setdefault("DEBUG", "true")


@pytest.fixture(scope="session")
def event_loop():
    """ç‚ºæ•´å€‹æ¸¬è©¦æœƒè©±å»ºç«‹äº‹ä»¶å¾ªç’°"""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()


@pytest.fixture
def mock_llm_response():
    """æ¨¡æ“¬ LLM å›æ‡‰"""
    return {
        "content": "é€™æ˜¯ä¸€å€‹æ¨¡æ“¬çš„ LLM å›æ‡‰",
        "usage": {
            "prompt_tokens": 100,
            "completion_tokens": 150,
            "total_tokens": 250
        },
        "model": "gpt-4o-mini"
    }


@pytest.fixture
def sample_agent_config():
    """ç¯„ä¾‹ Agent é…ç½®"""
    return {
        "role": "æ¸¬è©¦å°ˆå®¶",
        "goal": "åŸ·è¡Œå…¨é¢çš„è»Ÿé«”æ¸¬è©¦",
        "backstory": "å…·å‚™10å¹´æ¸¬è©¦ç¶“é©—çš„å°ˆæ¥­QAå·¥ç¨‹å¸«",
        "reasoning": True,
        "memory": True,
        "verbose": False  # æ¸¬è©¦æ™‚é—œé–‰è©³ç´°è¼¸å‡º
    }


@pytest.fixture
def mock_agent(sample_agent_config):
    """æ¨¡æ“¬ Agent"""
    agent = Mock(spec=Agent)
    agent.role = sample_agent_config["role"]
    agent.goal = sample_agent_config["goal"]
    agent.backstory = sample_agent_config["backstory"]
    agent.reasoning = sample_agent_config["reasoning"]
    agent.memory = sample_agent_config["memory"]
    agent.execute.return_value = "æ¨¡æ“¬ Agent åŸ·è¡Œçµæœ"
    return agent


@pytest.fixture
def sample_task_config():
    """ç¯„ä¾‹ Task é…ç½®"""
    return {
        "description": "åŸ·è¡Œå–®å…ƒæ¸¬è©¦ä¸¦ç”Ÿæˆå ±å‘Š",
        "expected_output": "åŒ…å«æ¸¬è©¦çµæœå’Œè¦†è“‹ç‡çš„è©³ç´°å ±å‘Š",
        "async_execution": False
    }


@pytest.fixture
def mock_task(sample_task_config, mock_agent):
    """æ¨¡æ“¬ Task"""
    task = Mock(spec=Task)
    task.description = sample_task_config["description"]
    task.expected_output = sample_task_config["expected_output"]
    task.agent = mock_agent
    task.async_execution = sample_task_config["async_execution"]
    
    # æ¨¡æ“¬åŸ·è¡Œçµæœ
    task.execute.return_value = Mock()
    task.execute.return_value.raw = "æ¸¬è©¦åŸ·è¡Œå®Œæˆï¼Œè¦†è“‹ç‡ 95%"
    task.execute.return_value.pydantic = None
    task.execute.return_value.json_dict = {"coverage": 95, "tests_passed": 120}
    
    return task


@pytest.fixture
def mock_tool():
    """æ¨¡æ“¬å·¥å…·"""
    tool = Mock(spec=BaseTool)
    tool.name = "æ¸¬è©¦å·¥å…·"
    tool.description = "ç”¨æ–¼æ¸¬è©¦çš„æ¨¡æ“¬å·¥å…·"
    tool.run.return_value = "å·¥å…·åŸ·è¡ŒæˆåŠŸ"
    return tool


@pytest.fixture
def sample_crew_config():
    """ç¯„ä¾‹ Crew é…ç½®"""
    return {
        "verbose": False,
        "memory": True,
        "planning": True
    }


@pytest.fixture
def mock_crew(mock_agent, mock_task, sample_crew_config):
    """æ¨¡æ“¬ Crew"""
    crew = Mock(spec=Crew)
    crew.agents = [mock_agent]
    crew.tasks = [mock_task]
    crew.verbose = sample_crew_config["verbose"]
    crew.memory = sample_crew_config["memory"]
    
    # æ¨¡æ“¬ kickoff çµæœ
    crew.kickoff.return_value = Mock()
    crew.kickoff.return_value.raw = "Crew åŸ·è¡Œå®Œæˆ"
    crew.kickoff.return_value.tasks_output = [mock_task.execute.return_value]
    crew.kickoff.return_value.token_usage = {
        "total_tokens": 500,
        "prompt_tokens": 200,
        "completion_tokens": 300
    }
    
    return crew


# Reflection Pattern æ¸¬è©¦ Fixtures
@pytest.fixture
def reflection_test_data():
    """åæ€æ¨¡å¼æ¸¬è©¦è³‡æ–™"""
    return {
        "initial_content": "é€™æ˜¯ä¸€å€‹éœ€è¦æ”¹é€²çš„åˆå§‹å…§å®¹ã€‚å…§å®¹è¼ƒç‚ºç°¡å–®ï¼Œç¼ºå°‘æ·±åº¦ã€‚",
        "reflection_criteria": [
            "æŠ€è¡“æº–ç¢ºæ€§",
            "å…§å®¹æ·±åº¦",
            "è¡¨é”æ¸…æ™°åº¦",
            "å¯¦ç”¨æ€§"
        ],
        "target_score": 8.0,
        "max_iterations": 3
    }


@pytest.fixture
def planning_test_data():
    """è¦åŠƒæ¨¡å¼æ¸¬è©¦è³‡æ–™"""
    return {
        "project_goal": "é–‹ç™¼ä¸€å€‹ CrewAI èŠå¤©æ©Ÿå™¨äºº",
        "constraints": {
            "timeline": "4é€±",
            "budget": "$10,000",
            "team_size": 3
        },
        "expected_tasks": [
            "éœ€æ±‚åˆ†æ",
            "ç³»çµ±è¨­è¨ˆ", 
            "é–‹ç™¼å¯¦ä½œ",
            "æ¸¬è©¦é©—è­‰",
            "éƒ¨ç½²ä¸Šç·š"
        ]
    }


@pytest.fixture
def tool_use_test_data():
    """å·¥å…·ä½¿ç”¨æ¨¡å¼æ¸¬è©¦è³‡æ–™"""
    return {
        "task_description": "æŸ¥è©¢æœ€æ–°çš„æ¯”ç‰¹å¹£åƒ¹æ ¼ä¸¦é€²è¡Œåˆ†æ",
        "available_tools": ["price_api", "data_analysis", "chart_generator"],
        "expected_workflow": [
            "ä½¿ç”¨ price_api ç²å–åƒ¹æ ¼",
            "ä½¿ç”¨ data_analysis åˆ†æè¶¨å‹¢",
            "ä½¿ç”¨ chart_generator ç”Ÿæˆåœ–è¡¨"
        ],
        "fallback_strategy": "ä½¿ç”¨å¿«å–æ•¸æ“š"
    }


@pytest.fixture
def multi_agent_test_data():
    """å¤šä»£ç†æ¨¡å¼æ¸¬è©¦è³‡æ–™"""
    return {
        "agents": [
            {"role": "ç ”ç©¶å“¡", "speciality": "æ•¸æ“šæ”¶é›†"},
            {"role": "åˆ†æå¸«", "speciality": "æ•¸æ“šåˆ†æ"},
            {"role": "å ±å‘Šå“¡", "speciality": "å ±å‘Šæ’°å¯«"}
        ],
        "collaboration_scenario": "å¸‚å ´ç ”ç©¶å ±å‘Š",
        "expected_interactions": [
            "ç ”ç©¶å“¡æ”¶é›†å¸‚å ´è³‡æ–™",
            "åˆ†æå¸«åˆ†æè³‡æ–™è¶¨å‹¢",
            "å ±å‘Šå“¡æ’°å¯«æœ€çµ‚å ±å‘Š"
        ]
    }


# æ•ˆèƒ½æ¸¬è©¦ Fixtures
@pytest.fixture
def performance_test_config():
    """æ•ˆèƒ½æ¸¬è©¦é…ç½®"""
    return {
        "concurrent_requests": 10,
        "test_duration": 30,  # ç§’
        "max_response_time": 5.0,  # ç§’
        "min_success_rate": 0.95
    }


@pytest.fixture
def benchmark_data():
    """åŸºæº–æ¸¬è©¦æ•¸æ“š"""
    return {
        "baseline_metrics": {
            "avg_response_time": 2.5,
            "token_usage_per_request": 200,
            "success_rate": 0.98,
            "cost_per_request": 0.01
        },
        "test_cases": [
            {"complexity": "low", "expected_time": 1.0},
            {"complexity": "medium", "expected_time": 2.5},
            {"complexity": "high", "expected_time": 5.0}
        ]
    }


# ç›£æ§æ¸¬è©¦ Fixtures
@pytest.fixture
def monitoring_test_data():
    """ç›£æ§æ¸¬è©¦è³‡æ–™"""
    return {
        "metrics": [
            "agent_execution_count",
            "task_completion_rate", 
            "error_rate",
            "token_usage",
            "response_time"
        ],
        "alerts": [
            {"metric": "error_rate", "threshold": 0.05, "severity": "high"},
            {"metric": "response_time", "threshold": 10.0, "severity": "medium"}
        ]
    }


# æ•´åˆæ¸¬è©¦ Fixtures
@pytest.fixture
def integration_test_env():
    """æ•´åˆæ¸¬è©¦ç’°å¢ƒ"""
    return {
        "database_url": "sqlite:///test.db",
        "redis_url": "redis://localhost:6379/1", 
        "chroma_host": "localhost",
        "chroma_port": 8001,
        "test_api_keys": {
            "openai": "test-openai-key",
            "anthropic": "test-anthropic-key"
        }
    }


@pytest.fixture(autouse=True)
def cleanup_test_data():
    """è‡ªå‹•æ¸…ç†æ¸¬è©¦è³‡æ–™"""
    yield
    
    # æ¸…ç†æ¸¬è©¦æª”æ¡ˆ
    test_files = [
        "test_output.json",
        "test_log.txt",
        "test_cache.pkl"
    ]
    
    for file in test_files:
        if os.path.exists(file):
            os.remove(file)


# åƒæ•¸åŒ–æ¸¬è©¦è³‡æ–™
@pytest.fixture(params=["gpt-4o", "gpt-4o-mini", "claude-3-haiku"])
def llm_models(request):
    """åƒæ•¸åŒ–çš„ LLM æ¨¡å‹åˆ—è¡¨"""
    return request.param


@pytest.fixture(params=["sequential", "hierarchical", "consensual"])
def process_types(request):
    """åƒæ•¸åŒ–çš„ Process é¡å‹"""
    return request.param


@pytest.fixture(params=[1, 3, 5])
def iteration_counts(request):
    """åƒæ•¸åŒ–çš„è¿­ä»£æ¬¡æ•¸"""
    return request.param


# è‡ªè¨‚æ¨™è¨˜
def pytest_configure(config):
    """é…ç½®è‡ªè¨‚ pytest æ¨™è¨˜"""
    config.addinivalue_line(
        "markers", "slow: mark test as slow running"
    )
    config.addinivalue_line(
        "markers", "integration: mark test as integration test"
    )
    config.addinivalue_line(
        "markers", "unit: mark test as unit test"
    )
    config.addinivalue_line(
        "markers", "reflection: mark test as reflection pattern test"
    )
    config.addinivalue_line(
        "markers", "planning: mark test as planning pattern test"
    )
    config.addinivalue_line(
        "markers", "tooluse: mark test as tool use pattern test"
    )
    config.addinivalue_line(
        "markers", "multiagent: mark test as multi-agent pattern test"
    )
    config.addinivalue_line(
        "markers", "performance: mark test as performance test"
    )


# æ¸¬è©¦æœƒè©±é‰¤å­
@pytest.fixture(scope="session", autouse=True)
def test_session_setup():
    """æ¸¬è©¦æœƒè©±è¨­å®š"""
    print("\nğŸš€ é–‹å§‹ CrewAI Agentic Patterns æ¸¬è©¦æœƒè©±")
    
    # è¨­å®šæ¸¬è©¦ç’°å¢ƒ
    os.environ["TESTING"] = "true"
    os.environ["LOG_LEVEL"] = "WARNING"  # é™ä½æ¸¬è©¦æ™‚çš„æ—¥èªŒå±¤ç´š
    
    yield
    
    print("\nâœ… CrewAI Agentic Patterns æ¸¬è©¦æœƒè©±å®Œæˆ")


# éŒ¯èª¤è™•ç†è¼”åŠ©å‡½æ•¸
@pytest.fixture
def assert_agent_execution():
    """æ–·è¨€ Agent åŸ·è¡Œçš„è¼”åŠ©å‡½æ•¸"""
    def _assert_execution(agent, task, expected_keywords=None):
        """
        æ–·è¨€ Agent åŸ·è¡Œçµæœ
        
        Args:
            agent: åŸ·è¡Œçš„ Agent
            task: åŸ·è¡Œçš„ Task  
            expected_keywords: æœŸæœ›åœ¨çµæœä¸­å‡ºç¾çš„é—œéµå­—
        """
        assert agent is not None, "Agent ä¸èƒ½ç‚ºç©º"
        assert task is not None, "Task ä¸èƒ½ç‚ºç©º"
        
        result = agent.execute(task.description)
        assert result is not None, "åŸ·è¡Œçµæœä¸èƒ½ç‚ºç©º"
        assert len(result) > 0, "åŸ·è¡Œçµæœä¸èƒ½ç‚ºç©ºå­—ä¸²"
        
        if expected_keywords:
            for keyword in expected_keywords:
                assert keyword in result, f"çµæœä¸­æ‡‰åŒ…å«é—œéµå­—: {keyword}"
        
        return result
    
    return _assert_execution


@pytest.fixture  
def mock_external_services():
    """æ¨¡æ“¬å¤–éƒ¨æœå‹™"""
    services = {
        "openai_api": Mock(),
        "serper_api": Mock(),
        "chroma_db": Mock(),
        "redis_cache": Mock()
    }
    
    # è¨­å®šé è¨­å›æ‡‰
    services["openai_api"].chat.completions.create.return_value = Mock()
    services["openai_api"].chat.completions.create.return_value.choices = [
        Mock(message=Mock(content="æ¨¡æ“¬çš„ OpenAI å›æ‡‰"))
    ]
    
    services["serper_api"].search.return_value = {
        "organic": [
            {"title": "æ¸¬è©¦çµæœ", "snippet": "é€™æ˜¯æ¸¬è©¦æœå°‹çµæœ"}
        ]
    }
    
    services["chroma_db"].query.return_value = {
        "documents": [["ç›¸é—œæ–‡ä»¶å…§å®¹"]],
        "distances": [[0.1]]
    }
    
    services["redis_cache"].get.return_value = None
    services["redis_cache"].set.return_value = True
    
    return services


# è³‡æ–™é©—è­‰è¼”åŠ©
@pytest.fixture
def validate_agentic_pattern():
    """é©—è­‰ Agentic Pattern å¯¦ä½œçš„è¼”åŠ©å‡½æ•¸"""
    def _validate_pattern(pattern_type: str, implementation: Any, expected_features: List[str]):
        """
        é©—è­‰ Agentic Pattern å¯¦ä½œ
        
        Args:
            pattern_type: æ¨¡å¼é¡å‹ (reflection/planning/tooluse/multiagent)
            implementation: å¯¦ä½œç‰©ä»¶
            expected_features: æœŸæœ›çš„åŠŸèƒ½ç‰¹æ€§
        """
        assert implementation is not None, f"{pattern_type} å¯¦ä½œä¸èƒ½ç‚ºç©º"
        
        for feature in expected_features:
            assert hasattr(implementation, feature), f"{pattern_type} æ‡‰å¯¦ä½œ {feature} åŠŸèƒ½"
        
        # æ ¹æ“šä¸åŒæ¨¡å¼é€²è¡Œç‰¹å®šé©—è­‰
        if pattern_type == "reflection":
            assert hasattr(implementation, "reflect_on_output"), "Reflection æ¨¡å¼æ‡‰æœ‰åæ€åŠŸèƒ½"
            assert hasattr(implementation, "iterative_refinement"), "Reflection æ¨¡å¼æ‡‰æœ‰è¿­ä»£æ”¹é€²åŠŸèƒ½"
            
        elif pattern_type == "planning":
            assert hasattr(implementation, "create_plan"), "Planning æ¨¡å¼æ‡‰æœ‰è¨ˆåŠƒå‰µå»ºåŠŸèƒ½"
            assert hasattr(implementation, "monitor_progress"), "Planning æ¨¡å¼æ‡‰æœ‰é€²åº¦ç›£æ§åŠŸèƒ½"
            
        elif pattern_type == "tooluse":
            assert hasattr(implementation, "execute_with_tools"), "Tool Use æ¨¡å¼æ‡‰æœ‰å·¥å…·åŸ·è¡ŒåŠŸèƒ½"
            assert hasattr(implementation, "execute_fallback_strategy"), "Tool Use æ¨¡å¼æ‡‰æœ‰å‚™ç”¨ç­–ç•¥"
            
        elif pattern_type == "multiagent":
            assert hasattr(implementation, "delegate_task"), "Multi-Agent æ¨¡å¼æ‡‰æœ‰ä»»å‹™å§”æ´¾åŠŸèƒ½"
            assert hasattr(implementation, "request_assistance"), "Multi-Agent æ¨¡å¼æ‡‰æœ‰å”åŠ©è«‹æ±‚åŠŸèƒ½"
    
    return _validate_pattern 